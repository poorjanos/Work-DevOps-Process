---
title: "DevOps Process Explanatory Data Analysis"
author: "János Poór"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
---


```{r knitr_init, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=FALSE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

```{r, message=FALSE, warning=FALSE}
library(here)
library(dplyr)
library(ggplot2)
library(lubridate)
library(scales)
library(tidyr)
library(purrr)
library(bupaR)
library(processmapR)
library(DiagrammeR)
library(forcats)
library(stringr)
```

```{r, message=FALSE, include=FALSE}
#########################################################################################
# Data Extraction #######################################################################
#########################################################################################

# Set JAVA_HOME, set max. memory, and load rJava library
Sys.setenv(JAVA_HOME = "C:\\Program Files\\Java\\jre1.8.0_171")
options(java.parameters = "-Xmx2g")
library(rJava)

# Output Java version
.jinit()
print(.jcall("java/lang/System", "S", "getProperty", "java.version"))

# Load RJDBC library
library(RJDBC)

# Get credentials
datamnr <-
  config::get("datamnr", file = "C:\\Users\\PoorJ\\Projects\\config.yml")

# Create connection driver
jdbcDriver <-
  JDBC(driverClass = "oracle.jdbc.OracleDriver", classPath = "C:\\Users\\PoorJ\\Desktop\\ojdbc7.jar")

# Open connection: kontakt---------------------------------------------------------------
jdbcConnection <-
  dbConnect(
    jdbcDriver,
    url = datamnr$server,
    user = datamnr$uid,
    password = datamnr$pwd
  )


# Get SQL scripts
readQuery <-
  function(file)
    paste(readLines(file, warn = FALSE), collapse = "\n")

# Fetch data
query_isd <- readQuery(here::here("SQL", "get_ISD_data.sql"))
t_isd <- dbGetQuery(jdbcConnection, query_isd)

query_timesheet <- readQuery(here::here("SQL", "get_workhours_per_issue.sql")) # closed cases only
t_timesheet <- dbGetQuery(jdbcConnection, query_timesheet)

query_timesheet_full <- readQuery(here::here("SQL", "get_workhours_per_issue_by_month.sql")) # open cases as well
t_timesheet_full <- dbGetQuery(jdbcConnection, query_timesheet_full)

t_mnap <- dbGetQuery(jdbcConnection, 'select * from t_mnap')

# Close db connection: kontakt
dbDisconnect(jdbcConnection)


# Transformations
t_isd <- t_isd %>% mutate(
  CREATED = ymd_hms(CREATED),
  TIMESTAMP = ymd_hms(TIMESTAMP),
  APPGROUP = case_when(
    stringr::str_detect(APPLICATION_GROUP_CONCAT, "/") ~ "Z_Közös",
    TRUE ~ APPLICATION_GROUP_CONCAT
  ),
  APPSINGLE = case_when(
    stringr::str_detect(APPLICATION, ";") ~ "Multiple",
    TRUE ~ "Single"
  )
)

t_timesheet <- t_timesheet %>% mutate(
  CREATED = ymd_hms(CREATED),
  APPGROUP = case_when(
    stringr::str_detect(APPLICATION_GROUP_CONCAT, "/") ~ "Z_Közös",
    TRUE ~ APPLICATION_GROUP_CONCAT
  ),
  APPSINGLE = case_when(
    stringr::str_detect(APPLICATION, ";") ~ "Multiple",
    TRUE ~ "Single"
  )
)

t_timesheet_full <- t_timesheet_full %>% mutate(
  CREATED = ymd_hms(CREATED),
  LOGGED_MONTH = ymd_hms(LOGGED_MONTH),
  APPGROUP = case_when(
    stringr::str_detect(APPLICATION_GROUP_CONCAT, "/") ~ "Z_Közös",
    TRUE ~ APPLICATION_GROUP_CONCAT
  ),
  APPSINGLE = case_when(
    stringr::str_detect(APPLICATION, ";") ~ "Multiple",
    TRUE ~ "Single"
  )
)

t_mnap <- t_mnap %>% mutate(IDOSZAK = ymd_hms(IDOSZAK))
```

# 1. Volumes

## 1.1. Number of Tickets Started and Closed

```{r, fig.width=10}
t_started <- t_isd %>%
  # Transform data
  filter(TIMESTAMP >= as.Date("2017-06-01") & TIMESTAMP <= as.Date("2019-05-31")) %>%
  mutate(EVENT_MONTH = floor_date(TIMESTAMP, unit = "month")) %>%
  group_by(CASE, CLASSIFICATION) %>%
  summarize(MONTH = min(EVENT_MONTH)) %>%
  ungroup() %>%
  group_by(MONTH, CLASSIFICATION) %>%
  summarize(STARTED = n()) %>%
  ungroup()

t_closed <- t_isd %>%
  # Transform data
  filter(TIMESTAMP >= as.Date("2017-06-01") & TIMESTAMP <= as.Date("2019-05-31")) %>%
  mutate(EVENT_MONTH = floor_date(TIMESTAMP, unit = "month")) %>%
  group_by(CASE, CLASSIFICATION) %>%
  summarize(MONTH = max(EVENT_MONTH)) %>%
  ungroup() %>%
  group_by(MONTH, CLASSIFICATION) %>%
  summarize(CLOSED = n()) %>%
  ungroup()

t_started %>% left_join(t_closed, by = c("MONTH", "CLASSIFICATION")) %>%
  tidyr::gather(-MONTH, -CLASSIFICATION, key = METRIC, value = COUNT) %>% 
  tidyr::replace_na(list(COUNT = 0)) %>%
  # Plot
  ggplot(aes(x = as.Date(MONTH), y = COUNT, group = METRIC, colour = METRIC)) +
  geom_line(size = 0.8) +
  scale_x_date(date_breaks = '3 months', date_labels = '%Y-%m') +
  theme(axis.text.x = element_text(angle = 90, size = 8)) +
  facet_grid(.~CLASSIFICATION, labeller = label_wrap_gen(width = 10)) +
  labs(
    x = "Month",
    y = "# of Tickets",
    title = "Number of Tickets Started and Closed"
  )
```


## 1.2. Number of Tickets Started and Closed by Application Group

```{r, fig.width=10}
t_started <- t_isd %>%
  # Transform data
  filter(TIMESTAMP >= as.Date("2017-06-01") & TIMESTAMP <= as.Date("2019-05-31")) %>%
  mutate(EVENT_MONTH = floor_date(TIMESTAMP, unit = "month")) %>%
  group_by(CASE, CLASSIFICATION, APPGROUP) %>%
  summarize(MONTH = min(EVENT_MONTH)) %>%
  ungroup() %>%
  group_by(MONTH, CLASSIFICATION, APPGROUP) %>%
  summarize(STARTED = n()) %>%
  ungroup()

t_closed <- t_isd %>%
  # Transform data
  filter(TIMESTAMP >= as.Date("2017-06-01") & TIMESTAMP <= as.Date("2019-05-31")) %>%
  mutate(EVENT_MONTH = floor_date(TIMESTAMP, unit = "month")) %>%
  group_by(CASE, CLASSIFICATION, APPGROUP) %>%
  summarize(MONTH = max(EVENT_MONTH)) %>%
  ungroup() %>%
  group_by(MONTH, CLASSIFICATION, APPGROUP) %>%
  summarize(CLOSED = n()) %>%
  ungroup()

t_started %>% left_join(t_closed, by = c("MONTH", "CLASSIFICATION", "APPGROUP")) %>%
  tidyr::gather(-MONTH, -CLASSIFICATION, -APPGROUP, key = METRIC, value = COUNT) %>% 
  tidyr::replace_na(list(COUNT = 0)) %>% 
  # Plot
  ggplot(aes(x = as.Date(MONTH), y = COUNT, group = METRIC, colour = METRIC)) +
  geom_line(size = 0.8) +
  scale_x_date(date_breaks = '3 months', date_labels = '%Y-%m') +
  theme(axis.text.x = element_text(angle = 90, size = 8),
        strip.text.y = element_text(angle = 0)) +
  facet_grid(APPGROUP~CLASSIFICATION, labeller = label_wrap_gen(width = 10)) +
  labs(
    x = "Month",
    y = "# of Tickets",
    title = "Number of Tickets Started and Closed by Application Group"
  )
```

# 2. Process Structure

``` {r}
# Add cols to event log required by BupaR
t_isd_eventlog <- t_isd %>%
  filter(TIMESTAMP >= as.Date("2017-06-01") & TIMESTAMP <= as.Date("2019-05-31")) %>%
  arrange(CASE, TIMESTAMP) %>%
  mutate(
    ACTIVITY_INST_ID = as.numeric(row.names(.)),
    LIFECYCLE_ID = "END"
  )


# Define analytics functions
# Trace number
get_trace_num <- function(df){
  number_of_traces(
    eventlog(
           df,
           case_id = "CASE",
           activity_id = "ACTIVITY",
           activity_instance_id = "ACTIVITY_INST_ID",
           lifecycle_id = "LIFECYCLE_ID",
           timestamp = "TIMESTAMP",
           resource_id = "RESOURCE"
           ))
}

# Trace coverage
get_trace_cov <- function(df){
  trace_coverage(
    eventlog(
      df,
      case_id = "CASE",
           activity_id = "ACTIVITY",
           activity_instance_id = "ACTIVITY_INST_ID",
           lifecycle_id = "LIFECYCLE_ID",
           timestamp = "TIMESTAMP",
           resource_id = "RESOURCE"
    ), level = "trace")
}


# Trace lenght aggregates
get_trace_length <- function(df) {
  tidyr::spread(
    data = data.frame(
      metric = c("mean", "median", "min", "max", "st_dev", "q1", "q3", "iqr"),
      values = trace_length(
        eventlog(
          df,
          case_id = "CASE",
           activity_id = "ACTIVITY",
           activity_instance_id = "ACTIVITY_INST_ID",
           lifecycle_id = "LIFECYCLE_ID",
           timestamp = "TIMESTAMP",
           resource_id = "RESOURCE"
        ),
        level = "log", units = "day"
      )[c("mean", "median", "min", "max", "st_dev", "q1", "q3", "iqr")], row.names = NULL
    ),
    key = metric, value = values
  )
}


# Trace lenght by case
get_trace_length_by_case <- function(df) {
  trace_length(
    eventlog(
      df,
      case_id = "CASE",
      activity_id = "ACTIVITY",
      activity_instance_id = "ACTIVITY_INST_ID",
      lifecycle_id = "LIFECYCLE_ID",
      timestamp = "TIMESTAMP",
      resource_id = "RESOURCE"
    ),
    level = "case", units = "day"
  )
}


# Throughput time
get_through_time <- function(df) {
  tidyr::spread(
    data = data.frame(
      metric = c("mean", "median", "min", "max", "st_dev", "q1", "q3"),
      values = throughput_time(
        eventlog(
          df,
          case_id = "CASE",
           activity_id = "ACTIVITY",
           activity_instance_id = "ACTIVITY_INST_ID",
           lifecycle_id = "LIFECYCLE_ID",
           timestamp = "TIMESTAMP",
           resource_id = "RESOURCE"
        ),
        level = "log", units = "day"
      )[c("mean", "median", "min", "max", "st_dev", "q1", "q3")], row.names = NULL
    ),
    key = metric, value = values
  )
}


# Throughput time by case
get_through_time_by_case <- function(df) {
    throughput_time(
      eventlog(
        df,
        case_id = "CASE",
        activity_id = "ACTIVITY",
        activity_instance_id = "ACTIVITY_INST_ID",
        lifecycle_id = "LIFECYCLE_ID",
        timestamp = "TIMESTAMP",
        resource_id = "RESOURCE"
      ),
      level = "case", units = "day"
    )
}


# Genarate nested df
# Gen nested tables with aggregated stats in nested tables
by_class_appgroup <- t_isd_eventlog %>%
  group_by(CLASSIFICATION, APPGROUP) %>%
  nest() %>%
  mutate(
    CASE_NUMBER = map(data, ~length(unique(.$CASE))),
    EVENT_NUMBER = map(data, ~length(unique(.$ACTIVITY))),
    TRACE_NUMBER = map(data, get_trace_num),
    TRACE_COV = map(data, get_trace_cov),
    TRACE_LENGTH_AGGREGATE = map(data, get_trace_length),
    TRACE_LENGTH_BY_CASE = map(data, get_trace_length_by_case),
    THROUGH_TIME_AGGREGATE = map(data, get_through_time),
    THROUGH_TIME_BY_CASE = map(data, get_through_time_by_case)
  )
```


## 2.1. Absolute Frequency of Ticket Types between 2017-06 and 2019-05

```{r, fig.width=10}
by_class_appgroup %>%
  # Transform data
  select(CLASSIFICATION, APPGROUP, CASE_NUMBER) %>%
  unnest() %>%
  replace_na(list(APPGROUP = "NA")) %>%
  mutate(
    APPGROUP =
      forcats::fct_relevel(
        factor(APPGROUP),
        "NA",
        after = Inf
      )
  ) %>%
  # Plot
  ggplot(aes(
    x = APPGROUP,
    y = CASE_NUMBER,
    group = CLASSIFICATION,
    fill = CLASSIFICATION
  )) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = CASE_NUMBER), position = position_stack(vjust = 0.5), size = 3) +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(
    x = "Application Group",
    y = "Absolute Frequency (count)",
    title = "Absolute Frequency of Ticket Types (2 years span)",
    fill = "Ticket Type"
  )
```


## 2.2. Relative Frequency of Ticket Types between 2017-06 and 2019-05

```{r, fig.width=10}
by_class_appgroup %>%
  # Transform data
  select(CLASSIFICATION, APPGROUP, CASE_NUMBER) %>%
  unnest() %>%
  replace_na(list(APPGROUP = "NA")) %>%
  mutate(
    APPGROUP =
      forcats::fct_relevel(
        factor(APPGROUP),
        "NA",
        after = Inf
      )
  ) %>%
  arrange(APPGROUP, CLASSIFICATION) %>%
  group_by(APPGROUP) %>%
  mutate(TOTAL_CASE_NUM = sum(CASE_NUMBER)) %>%
  ungroup() %>%
  mutate(RELATIVE_FREQ = CASE_NUMBER / TOTAL_CASE_NUM) %>%
  # Plot
  ggplot(aes(
    x = APPGROUP,
    y = RELATIVE_FREQ,
    group = CLASSIFICATION,
    fill = CLASSIFICATION
  )) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = scales::percent(RELATIVE_FREQ)), position = position_stack(vjust = 0.5), size = 3) +
  scale_y_continuous(labels = percent) +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(
    x = "Application Group",
    y = "Relative Frequency (percent)",
    title = "Relative Frequency of Ticket Types",
    fill = "Ticket Type"
  )
```


## 2.3. Relative Trace Number (Process Complexity Based on Trace Variation)

Relative trace coverage gives an insight to the complexity of the underlying process. The more traces needed to cover a 100 cases the higher the variation of traces, thus the more complex the process.

```{r, fig.width=10}
by_class_appgroup %>%
  # Transform data
  select(CLASSIFICATION, APPGROUP, TRACE_NUMBER) %>%
  unnest() %>%
  replace_na(list(APPGROUP = "NA")) %>%
  mutate(APPGROUP = forcats::fct_rev(
    forcats::fct_relevel(
      factor(APPGROUP),
      "NA",
      after = Inf
    )
  )) %>%
  # Plot
  ggplot(aes(
    x = APPGROUP,
    y = average_coverage
  )) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 0)) +
  coord_flip() +
  facet_grid(. ~ CLASSIFICATION, labeller = label_wrap_gen(width = 8)) +
  labs(
    x = "Application Group",
    y = "Average Coverage",
    title = "Relative Trace Number (No. of Traces to Cover 100 Cases)"
  )
```


## 2.4. Trace Coverage

Trace coverage distribution is another angle on process complexity. It shows the distribution of unique traces covering covering the process. The faster the line reaches 100% the lower is the number of unique traces, thus the less complex the process.

```{r,  fig.width=10}
by_class_appgroup %>%
  # Transform data
  select(CLASSIFICATION, APPGROUP, TRACE_COV) %>% 
  unnest() %>% 
  group_by(CLASSIFICATION, APPGROUP) %>%
  mutate(RNUM = row_number()) %>%
  ungroup() %>%
  # Plot
  ggplot(aes(x = RNUM, y = cum_sum)) +
      geom_line() +
      theme(axis.text.x = element_text(angle = 90),
            strip.text.y = element_text(angle = 0)) +
      coord_cartesian(xlim = c(0, 500)) +
      facet_grid(APPGROUP~CLASSIFICATION, labeller = label_wrap_gen(width = 8)) +
      labs(
          x = "Unique traces",
          y = "Coverage (Cumulative Perentage)",
          title = "Trace Coverage By Application Group"
        )
```


# 3. Performance Statistics


## 3.1. Throughput Time Recent Performance (medians in day 2019)

An overview of throughput times from the last five months.

```{r,  fig.width=10}
# Compute starting month for each case
case_month <- t_isd_eventlog %>% 
  group_by(CASE) %>%
  summarize(START_MONTH = floor_date(min(TIMESTAMP), unit="months")) %>% 
  ungroup()

by_class_appgroup %>%
  # Transform data
  select(CLASSIFICATION, APPGROUP, THROUGH_TIME_BY_CASE) %>%
  unnest() %>%
  replace_na(list(APPGROUP = "NA")) %>%
  mutate(APPGROUP = forcats::fct_rev(
    forcats::fct_relevel(
      factor(APPGROUP),
      "NA",
      after = Inf
    )
  )) %>%
  left_join(case_month, by = "CASE") %>% 
  filter(START_MONTH >= as.Date("2019-01-01") & START_MONTH <= as.Date("2019-06-01")) %>%
  group_by(CLASSIFICATION, APPGROUP) %>% 
  summarize(THROUGHPUT_TIME_MEDIAN = median(throughput_time),
             THROUGHPUT_TIME_AVG = mean(throughput_time)) %>% 
  ungroup() %>%
  # Plot
  ggplot(aes(
    x = APPGROUP,
    y = THROUGHPUT_TIME_MEDIAN
  )) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 0)) +
  coord_flip() +
  facet_grid(. ~ CLASSIFICATION, labeller = label_wrap_gen(width = 8)) +
  labs(
    x = "Application Group",
    y = "Median Throughput Time (days)",
    title = "Median Throughput Times (2019)"
  )
```


## 3.2. Throughput Time Two Years Seasonality

No apparent seasonal trends, however, process in general appears more stable in 2019.

```{r,  fig.width=10}
by_class_appgroup %>%
  # Transform data
  select(CLASSIFICATION, APPGROUP, THROUGH_TIME_BY_CASE) %>%
  unnest() %>%
  left_join(case_month, by = "CASE") %>% 
  group_by(CLASSIFICATION, APPGROUP, START_MONTH) %>% 
  summarize(THROUGHPUT_TIME_MEDIAN = median(throughput_time),
             THROUGHPUT_TIME_AVG = mean(throughput_time)) %>% 
  ungroup() %>%
  # Plot
  ggplot(
    aes(x = START_MONTH,
        y = THROUGHPUT_TIME_MEDIAN)) +
  geom_line() +
  theme(axis.text.x = element_text(angle = 90),
        strip.text.y = element_text(angle = 0)) +
  facet_grid(APPGROUP~CLASSIFICATION, labeller = label_wrap_gen(width = 10), scales = "free") +
  labs(
    y = "Throughput Time (day)",
    x = "Month",
    title = "Median of Throughput Time by Month"
  )
```


## 3.3. Throughput Time Distributions (2019)

Strange bimodalitiy in case of INC may refer to a fast-paced happy-flow that is treated with either higher prioriry or with less steps. Needs process mining to confirm.

```{r,  fig.width=10}
by_class_appgroup %>%
  # Transform data
  select(CLASSIFICATION, APPGROUP, THROUGH_TIME_BY_CASE) %>%
  unnest() %>%
  left_join(case_month, by = "CASE") %>% 
  filter(START_MONTH >= as.Date("2019-01-01") & START_MONTH <= as.Date("2019-06-01")) %>%
  group_by(CLASSIFICATION, APPGROUP) %>% 
  ungroup() %>%
  # Plot
  ggplot(
      aes(x = throughput_time)) +
    geom_histogram(bins = 50) +
    coord_cartesian(xlim = c(0, 50)) +
    theme(axis.text.x = element_text(angle = 0),
          strip.text.y = element_text(angle = 0)) +
    facet_grid(APPGROUP~CLASSIFICATION, labeller = label_wrap_gen(width = 10)) +
    labs(
      y = "Count",
      x = "Throughput Time (days)",
      title = "Throughput Time Distribution"
    )
```


## 3.4. Trace Length Recent Performance (median num of steps 2019)

```{r, fig.width=10}
by_class_appgroup %>%
  # Transform data
  select(CLASSIFICATION, APPGROUP, TRACE_LENGTH_BY_CASE) %>%
  unnest() %>%
  replace_na(list(APPGROUP = "NA")) %>%
  mutate(APPGROUP = forcats::fct_rev(
    forcats::fct_relevel(
      factor(APPGROUP),
      "NA",
      after = Inf
    )
  )) %>%
  left_join(case_month, by = "CASE") %>% 
  filter(START_MONTH >= as.Date("2019-01-01") & START_MONTH <= as.Date("2019-06-01")) %>%
  group_by(CLASSIFICATION, APPGROUP) %>% 
  summarize(TRACE_LENGTH_MEDIAN = median(trace_length),
             TRACE_LENGTH_AVG = mean(trace_length)) %>% 
  ungroup() %>%
  # Plot
  ggplot(aes(
    x = APPGROUP,
    y = TRACE_LENGTH_MEDIAN
  )) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 0)) +
  coord_flip() +
  facet_grid(. ~ CLASSIFICATION, labeller = label_wrap_gen(width = 8)) +
  labs(
    x = "Application Group",
    y = "Median Trace Lenght (num of steps)",
    title = "Median Trace Length (2019)"
  )
```


## 3.5. Trace Lenght Two Years Seasonality

Low variability of trace lenght in time suggest standardized process flows for INC. RFC and DEV processes show higher trace length variability.

```{r,  fig.width=10}
by_class_appgroup %>%
  # Transform data
  select(CLASSIFICATION, APPGROUP, TRACE_LENGTH_BY_CASE) %>%
  unnest() %>%
  left_join(case_month, by = "CASE") %>% 
  group_by(CLASSIFICATION, APPGROUP, START_MONTH) %>% 
  summarize(TRACE_LENGTH_MEDIAN = median(trace_length),
             TRACE_LENGTH_AVG = mean(trace_length)) %>% 
  ungroup() %>%
  # Plot
  ggplot(
    aes(x = START_MONTH,
        y = TRACE_LENGTH_MEDIAN)) +
  geom_line() +
  theme(axis.text.x = element_text(angle = 90),
        strip.text.y = element_text(angle = 0)) +
  facet_grid(APPGROUP~CLASSIFICATION, labeller = label_wrap_gen(width = 10), scales = "free") +
  labs(
    y = "Trace Lenght (num of steps)",
    x = "Month",
    title = "Median of Trace Length by Month"
  )
```


## 3.6. Trace Lenght Distributions (2019)

INC shows strange bimodality. Need to compare process branches.

```{r,  fig.width=10}
by_class_appgroup %>%
  # Transform data
  select(CLASSIFICATION, APPGROUP, TRACE_LENGTH_BY_CASE) %>%
  unnest() %>%
  left_join(case_month, by = "CASE") %>% 
  filter(START_MONTH >= as.Date("2019-01-01") & START_MONTH <= as.Date("2019-06-01")) %>%
  group_by(CLASSIFICATION, APPGROUP) %>% 
  ungroup() %>%
  # Plot
  ggplot(
      aes(x = trace_length)) +
    geom_histogram(bins = 30) +
    coord_cartesian(xlim = c(0, 50)) +
    theme(axis.text.x = element_text(angle = 0),
          strip.text.y = element_text(angle = 0)) +
    facet_grid(APPGROUP~CLASSIFICATION, labeller = label_wrap_gen(width = 10)) +
    labs(
      y = "Count",
      x = "Trace Length",
      title = "Trace Lenght Distribution"
    )
```


# 4. INC Deep Dive

Process mining reveals root-cause of bimodalitiy of INC throughput distribution: some INC tickets do not flow through the process, but are aborted after report. 

```{r, include=FALSE}
# Subset aborted cases
abort_events <- c("H09", "H10", "H11", "H12", "H13", "H14")

t_INC_to_filter <- t_isd %>%
  filter(TIMESTAMP >= as.Date("2017-06-01") & TIMESTAMP <= as.Date("2019-05-31")) %>%
  filter(CLASSIFICATION == "Hibabejelentés (INC)") %>%
  filter(stringr::str_detect(ACTIVITY, paste(abort_events, collapse = "|"))) %>% 
  distinct(CASE)


# Filter for aborted cases then add cols to event log required by BupaR
t_isd_eventlog_INC_noAbort <- t_isd %>%
  filter(TIMESTAMP >= as.Date("2017-06-01") & TIMESTAMP <= as.Date("2019-05-31")) %>%
  arrange(CASE, TIMESTAMP) %>%
  filter(!CASE %in% t_INC_to_filter$CASE) %>% 
  mutate(
    ACTIVITY_INST_ID = as.numeric(row.names(.)),
    LIFECYCLE_ID = "END"
  )


# Genarate nested df
# Gen nested tables with aggregated stats in nested tables
by_class_appgroup_INC_noAbort <- t_isd_eventlog_INC_noAbort %>%
  group_by(CLASSIFICATION, APPGROUP) %>%
  nest() %>%
  mutate(
    CASE_NUMBER = map(data, ~length(unique(.$CASE))),
    EVENT_NUMBER = map(data, ~length(unique(.$ACTIVITY))),
    TRACE_NUMBER = map(data, get_trace_num),
    TRACE_COV = map(data, get_trace_cov),
    TRACE_LENGTH_AGGREGATE = map(data, get_trace_length),
    TRACE_LENGTH_BY_CASE = map(data, get_trace_length_by_case),
    THROUGH_TIME_AGGREGATE = map(data, get_through_time),
    THROUGH_TIME_BY_CASE = map(data, get_through_time_by_case)
  )
```


## 4.1. INC throughput times without aborted INC tickets

```{r, fig.width=8.5, fig.height = 5.5}
# Compute starting month for each case
case_month <- t_isd_eventlog %>% 
  group_by(CASE) %>%
  summarize(START_MONTH = floor_date(min(TIMESTAMP), unit="months")) %>% 
  ungroup()


by_class_appgroup_INC_noAbort %>% 
  # Transform data
  select(CLASSIFICATION, APPGROUP, THROUGH_TIME_BY_CASE) %>%
  unnest() %>%
  replace_na(list(APPGROUP = "NA")) %>%
  filter(APPGROUP != "NA") %>% 
  mutate(APPGROUP = forcats::fct_rev(forcats::fct_recode(APPGROUP, Közös = "Z_Közös"))) %>% 
  left_join(case_month, by = "CASE") %>% 
  filter(START_MONTH >= as.Date("2019-01-01") & START_MONTH <= as.Date("2019-06-01")) %>%
  group_by(CLASSIFICATION, APPGROUP) %>% 
  summarize(THROUGHPUT_TIME_MEDIAN = median(throughput_time),
             THROUGHPUT_TIME_AVG = mean(throughput_time)) %>% 
  ungroup() %>%
  # Plot
  ggplot(aes(
    x = APPGROUP,
    y = THROUGHPUT_TIME_MEDIAN
  )) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(THROUGHPUT_TIME_MEDIAN, 2)), size = 3, hjust = 0) +
  theme(axis.text.x = element_text(angle = 90),
        strip.text.y = element_text(angle = 0)) +
  coord_flip(ylim = c(0, 80)) +
  facet_wrap(~CLASSIFICATION, nrow = 2, labeller = label_wrap_gen(width = 18)) +
  labs(
    x = "Application Group",
    y = "Median Throughput Time (days)",
    title = "Median Throughput Times (2019)"
  )
```

## 4.2. Throughput Time Distributions without aborted INC tickets (2019)

```{r,  fig.width=10}
by_class_appgroup_INC_noAbort %>%
  # Transform data
  select(CLASSIFICATION, APPGROUP, THROUGH_TIME_BY_CASE) %>%
  unnest() %>%
  left_join(case_month, by = "CASE") %>% 
  filter(START_MONTH >= as.Date("2019-01-01") & START_MONTH <= as.Date("2019-06-01")) %>%
  group_by(CLASSIFICATION, APPGROUP) %>% 
  ungroup() %>%
  # Plot
  ggplot(
      aes(x = throughput_time)) +
    geom_histogram(bins = 50) +
    coord_cartesian(xlim = c(0, 50)) +
    theme(axis.text.x = element_text(angle = 0),
          strip.text.y = element_text(angle = 0)) +
    facet_grid(APPGROUP~CLASSIFICATION, labeller = label_wrap_gen(width = 10)) +
    labs(
      y = "Count",
      x = "Throughput Time (days)",
      title = "Throughput Time Distribution"
    )
```


# 5. Timesheet Missing Value Patterns

Timesheet hours missing before Oct, 2018

```{r, include = FALSE}
sum(is.na(t_timesheet$HOURS_WORKTIMESHEET)) / length(t_timesheet$HOURS_WORKTIMESHEET)
```

```{r, fig.width=11}
t_timesheet_full %>%
  # Transform data
  select(-LOGGED_MONTH) %>% 
  group_by(CASE, CREATED, APPGROUP) %>% 
  summarize(HOURS_WORKTIMESHEET = sum(HOURS_WORKTIMESHEET)) %>% 
  ungroup() %>% 
  filter(CREATED >= as.Date("2017-01-01") & CREATED <= as.Date("2019-05-31")) %>%
  mutate(CREATED = as.Date(floor_date(CREATED, unit = "month"))) %>%
  group_by(CREATED, APPGROUP) %>%
  summarize(TIMESHEET_MISSING = 1 - sum(is.na(HOURS_WORKTIMESHEET)) / length(HOURS_WORKTIMESHEET)) %>%
  ungroup() %>%
  replace_na(list(APPGROUP = "NA")) %>%
  filter(APPGROUP != "NA") %>%
  mutate(APPGROUP = forcats::fct_recode(APPGROUP, Közös = "Z_Közös")) %>%
  # Plot
  ggplot(aes(
    x = CREATED,
    y = TIMESHEET_MISSING,
    group = 1
  )) +
  geom_line() +
  scale_x_date(date_breaks = '3 months', date_labels = '%Y-%m') +
  scale_y_continuous(labels = percent) +
  theme(
    axis.text.x = element_text(angle = 90),
    strip.text.y = element_text(angle = 0)
  ) +
  facet_grid(.~APPGROUP, labeller = label_wrap_gen(width = 14)) +
  labs(
    x = "Application Group",
    y = "Ratio of Tickets with Logged Workhours",
    title = "Ratio of Tickets with Logged Workhours"
  )
```


## 5.1. FTE captured on timesheets

```{r, fig.width=11}
t_timesheet_full %>%
  # Transform data
  filter(!is.na(LOGGED_MONTH)) %>% 
  filter(LOGGED_MONTH >= as.Date("2017-01-01") & LOGGED_MONTH <= as.Date("2019-05-31")) %>%
  mutate(LOGGED_MONTH = as.POSIXct(floor_date(LOGGED_MONTH, unit = "month"))) %>%
  group_by(LOGGED_MONTH, APPGROUP) %>%
  summarize(HOURS_WORKTIMESHEET = sum(HOURS_WORKTIMESHEET, na.rm = TRUE)) %>%
  ungroup() %>%
  left_join(t_mnap, by = c("LOGGED_MONTH" = "IDOSZAK")) %>% 
  filter(!is.na(APPGROUP)) %>% 
  mutate(APPGROUP = forcats::fct_recode(APPGROUP, Közös = "Z_Közös"),
         FTE = HOURS_WORKTIMESHEET/7/MNAP) %>%
  # Plot
  ggplot(aes(
    x = as.Date(LOGGED_MONTH),
    y = round(FTE, 2),
    group = 1
  )) +
  geom_line() +
  scale_x_date(date_breaks = '1 months', date_labels = '%Y-%m') +
  #scale_y_continuous(labels = percent) +
  theme(
    axis.text.x = element_text(angle = 90),
    strip.text.y = element_text(angle = 0)
  ) +
  facet_grid(.~APPGROUP, labeller = label_wrap_gen(width = 14)) +
  labs(
    x = "Month",
    y = "FTE",
    title = "FTE per Month"
  )
```
